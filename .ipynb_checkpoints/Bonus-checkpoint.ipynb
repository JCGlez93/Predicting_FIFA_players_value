{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Machine Learning Algorithms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Model Selection and Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Performance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# For Missing Values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Processsing\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing raw data\n",
    "\n",
    "\n",
    "data_2016 = pd.read_csv(\"/home/jc/Escritorio/Data_source_final_project/players_16.csv\")\n",
    "data_2017 = pd.read_csv(\"/home/jc/Escritorio/Data_source_final_project/players_17.csv\")\n",
    "data_2018 = pd.read_csv(\"/home/jc/Escritorio/Data_source_final_project/players_18.csv\")\n",
    "data_2019 = pd.read_csv(\"/home/jc/Escritorio/Data_source_final_project/players_18.csv\")\n",
    "data_2020 = pd.read_csv(\"/home/jc/Escritorio/Data_source_final_project/players_20.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick look at data\n",
    "data_2016.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataframes\n",
    "\n",
    "data_2016 = pd.read_csv(\"/home/jc/Escritorio/Data_source_final_project/players_16.csv\")\n",
    "data_2017 = pd.read_csv(\"/home/jc/Escritorio/Data_source_final_project/players_17.csv\")\n",
    "data_2018 = pd.read_csv(\"/home/jc/Escritorio/Data_source_final_project/players_18.csv\")\n",
    "data_2019 = pd.read_csv(\"/home/jc/Escritorio/Data_source_final_project/players_18.csv\")\n",
    "data_2020 = pd.read_csv(\"/home/jc/Escritorio/Data_source_final_project/players_20.csv\")\n",
    "\n",
    "\n",
    "print(data_2016.shape)\n",
    "\n",
    "\n",
    "data_2016['year'] = 2016\n",
    "data_2017['year'] = 2017\n",
    "data_2018['year'] = 2018\n",
    "data_2019['year'] = 2019\n",
    "data_2020['year'] = 2020\n",
    "\n",
    "raw_data = pd.concat([data_2016,data_2017,data_2018,data_2019,data_2020] , ignore_index= True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving visualisation for tableau\n",
    "\n",
    "mainpath = \"/home/jc/Escritorio/Data_source_final_project/\"\n",
    "filename =\"raw_data.\"\n",
    "fullpath = os.path.join(mainpath, filename)\n",
    "raw_data.to_csv(fullpath+\"csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for correlations on numeric variables\n",
    "\n",
    "plt.figure(figsize=(80, 50))\n",
    "\n",
    "sns_plot = sns.heatmap ( raw_data.corr(), annot = True )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the plot for future visualisation\n",
    "\n",
    "fig = sns_plot.get_figure()\n",
    "fig.savefig(\"output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Data Wrangling : cleaning data to apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "raw_data.select_dtypes(include =['object'] ).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data.select_dtypes(include =['object'] ).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_with_dummies = pd.get_dummies(raw_data, columns=[ 'nationality',\n",
    " 'club',\n",
    " 'player_positions',\n",
    " 'preferred_foot',\n",
    " 'work_rate'])\n",
    "df_with_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From our previous analysis , we know that there are some Nans. Our model may not work so we must modify it\n",
    "#From our previous analysis , we know that there are some Nans. Our model may not work so we must modify it\n",
    "\n",
    "#print(raw_data.info())\n",
    "selection = df_with_dummies.select_dtypes(exclude =['object'] )\n",
    "#print(selection.info())\n",
    "print(selection.shape)\n",
    "\n",
    "\n",
    "selection = selection.fillna( value = selection.mean())\n",
    "print(selection.shape)\n",
    "selection = selection.fillna( value = 0)\n",
    "print(selection.shape)\n",
    "selection = selection[selection.value_eur != 0]\n",
    "print(selection.shape)\n",
    "\n",
    "\n",
    "print(f'selection shape: {selection.shape}')\n",
    "print(f'raw_data shape:{raw_data.shape}')\n",
    "\n",
    "print(f'Dropped rows : {len(raw_data) - len(selection)}' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying all columns\n",
    "pd.options.display.max_columns = None\n",
    "selection.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_train =  selection[selection.year != 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selection_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selection_train['year'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = selection_train.columns.values.tolist()\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.remove('value_eur')\n",
    "print(len(a))\n",
    "\n",
    "a.remove('release_clause_eur')\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = selection_train[a]\n",
    "\n",
    "y_train = selection_train['value_eur']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_test =  selection[selection.year == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selection_test['year'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = selection_test[a]\n",
    "\n",
    "y_test = selection_test['value_eur']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we are using temporal data , I am splitting 2015-2019 data to train and 2020 data to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=30, random_state=42)\n",
    "forest_reg.fit(X_train, y_train)\n",
    "\n",
    "forest_reg.feature_importances_\n",
    "\n",
    "#creating a dataframe with coeficients\n",
    "display(pd.DataFrame(forest_reg.feature_importances_ , X_train.columns , columns = ['Coeff']).sort_values( by = ['Coeff'] , ascending = False).head(30))\n",
    "\n",
    "\n",
    "prediction_randomforest = forest_reg.predict(X_test)\n",
    "\n",
    "#prediction_randomforest = forest_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "forest_mse = mean_squared_error(y_test, prediction_randomforest)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = r2_score(y_test, prediction_randomforest)  \n",
    "print('r2:',format(score*100,'.2f'),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_scatter = plt.scatter (y_test , prediction_randomforest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'y_test' : y_test , 'predictions': prediction_randomforest}\n",
    "df = pd.DataFrame(d)\n",
    "df['difference'] = ((y_test - prediction_randomforest) / y_test) * 100\n",
    "a = df['difference'].mean()\n",
    "df.reset_index(inplace = True) #quitar el antiguo\n",
    "\n",
    "print(f'Media de la diferencia entre predicción y valor real:{a}')\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "#Saving visualisation for tableau\n",
    "\n",
    "mainpath = \"/home/jc/Escritorio/Data_source_final_project/\"\n",
    "filename =\"prediction_randomforest.\"\n",
    "fullpath = os.path.join(mainpath, filename)\n",
    "df.to_csv(fullpath+\"csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predictions'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predictions'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "prediction_decision_tree = tree_reg.predict(X_test)\n",
    "tree_mse = mean_squared_error(y_test, prediction_decision_tree)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse\n",
    "\n",
    "#creating a dataframe with coeficients\n",
    "display(pd.DataFrame(tree_reg.feature_importances_ , X_train.columns , columns = ['Coeff']).sort_values( by = ['Coeff'] , ascending = False).head(30))\n",
    "\n",
    "\n",
    "\n",
    "score = r2_score(y_test, prediction_decision_tree)  \n",
    "print('Accuracy:',format(score*100,'.2f'),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_scatter = plt.scatter (y_test , prediction_randomforest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'y_test' : y_test , 'predictions': prediction_decision_tree}\n",
    "df = pd.DataFrame(d)\n",
    "df['difference'] = ((y_test - prediction_decision_tree) / y_test) * 100\n",
    "a = df['difference'].mean()\n",
    "df.reset_index(inplace = True) #quitar el antiguo\n",
    "\n",
    "print(f'Media de la diferencia entre predicción y valor real:{a}')\n",
    "\n",
    "df.tail()\n",
    "\n",
    "#Saving visualisation for tableau\n",
    "\n",
    "mainpath = \"/home/jc/Escritorio/Data_source_final_project/\"\n",
    "filename =\"prediction_decisiontree.\"\n",
    "fullpath = os.path.join(mainpath, filename)\n",
    "df.to_csv(fullpath+\"csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predictions'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predictions'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-Linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression() #model that we are going to use\n",
    "linear_reg.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe with coeficients\n",
    "pd.DataFrame(linear_reg.coef_ , X_train.columns , columns = ['Coeff']).sort_values( by = ['Coeff'] , ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the model values that he has never seen (X_test)\n",
    "\n",
    "prediction_linear_reg = linear_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing our prediction to real values on a plot (visually)\n",
    "\n",
    "plt.scatter (y_test , prediction_linear_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'y_test' : y_test , 'predictions': prediction_linear_reg}\n",
    "df = pd.DataFrame(d)\n",
    "df['difference'] = ((y_test - prediction_linear_reg) / y_test) * 100\n",
    "a = df['difference'].mean()\n",
    "df.reset_index(inplace = True) #quitar el antiguo\n",
    "\n",
    "print(f'Media de la diferencia entre predicción y valor real:{a}')\n",
    "\n",
    "df.tail()\n",
    "\n",
    "#Saving visualisation for tableau\n",
    "\n",
    "mainpath = \"/home/jc/Escritorio/Data_source_final_project/\"\n",
    "filename =\"prediction_linearreg.\"\n",
    "fullpath = os.path.join(mainpath, filename)\n",
    "df.to_csv(fullpath+\"csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.mean_absolute_error(y_test, prediction_linear_reg)\n",
    "metrics.mean_squared_error(y_test , prediction_linear_reg)\n",
    "np.sqrt(metrics.mean_squared_error(y_test , prediction_linear_reg))\n",
    "metrics.r2_score(y_test, prediction_linear_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = r2_score(y_test, prediction_linear_reg)  \n",
    "print('Accuracy:',format(score*100,'.2f'),'%')\n",
    "\n",
    "\n",
    "#Saving visualisation for tableau\n",
    "\n",
    "mainpath = \"/home/jc/Escritorio/Data_source_final_project/\"\n",
    "filename =\"prediction_linearreg.\"\n",
    "fullpath = os.path.join(mainpath, filename)\n",
    "df.to_csv(fullpath+\"csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-Evaluating using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(forest_reg, X_train, y_train,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(linear_reg, X_train, y_train,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_scores = cross_val_score(tree_reg, X_train, y_train,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-tree_scores)\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-Fine-tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1-Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 40], 'max_features': [2, 3, 6]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "final_predictions = final_model.predict(X_test)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_score = r2_score(y_test, final_predictions)  \n",
    "print('Accuracy:',format(final_model_score*100,'.2f'),'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "\n",
    "import datetime\n",
    "try:\n",
    "    # for Python2\n",
    "    from Tkinter import *   ## notice capitalized T in Tkinter\n",
    "except ImportError:\n",
    "    # for Python3\n",
    "    from tkinter import *   ## notice lowercase 't' in tkinter here\n",
    "Current_Date = datetime.datetime.today() \n",
    "Current_Date= str(Current_Date)\n",
    "\n",
    "a = X_train.columns.values.tolist()\n",
    "root=Tk()\n",
    "\n",
    "root.title(\"Fifa tool prediction\")\n",
    "entries = []\n",
    "cont=0\n",
    "\n",
    "#label3 = Label(root, text= display(X_train.describe())).grid(row = 7 , column = 100 )\n",
    "for y in (a):\n",
    "       \n",
    "    my_label = Label(root, text = a[cont] )\n",
    "    my_label.grid(row = cont ,column= 4 , pady=20 )\n",
    "    my_label.config(font=(\"Arial\", 10))\n",
    "    cont = cont +1\n",
    "\n",
    "for i in range(len(a)):\n",
    "    en = Entry(root)\n",
    "    en.grid(row=i, column=0)\n",
    "    entries.append(en)\n",
    "    i = i +1\n",
    "\n",
    "def execute():\n",
    "    lista= []\n",
    "    for entry in entries:\n",
    "        #print(entry.get())\n",
    "        lista.append(entry.get())\n",
    "    #print(lista)\n",
    "    df = pd.DataFrame(columns=a )\n",
    "    df.loc[0] = lista\n",
    "    display(df)\n",
    "    print(type(df))\n",
    "    prediction_randomforest = forest_reg.predict(df)\n",
    "    prediction_randomforest = int(prediction_randomforest)\n",
    "    display(prediction_randomforest)\n",
    "    print(type(prediction_randomforest))\n",
    "    \n",
    "    label = Label(root, text= f'Hola, su programa ha sido ejecutado con éxito en el dia {Current_Date}').grid(row = 5 , column = 1000 )\n",
    "    #label.config(font=(\"Arial\", 12))\n",
    "    label2 = Label(root, text= f'El valor de este jugador es:  {prediction_randomforest} €').grid(row = 7 , column = 1000 )\n",
    "    \n",
    "    \n",
    "\n",
    "button=Button(root,text=\"Execute model\",command=execute).grid(row = 10 , column = 1000 )\n",
    "\n",
    "root.mainloop()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter_env] *",
   "language": "python",
   "name": "conda-env-jupyter_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
